import torch
import tempfile
import requests
import os
import safetensors.torch

def safe_load_state(path):
    """
    ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå model ‡∏à‡∏≤‡∏Å path ‡πÇ‡∏î‡∏¢‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ä‡∏ô‡∏¥‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥
    ‡∏£‡∏≠‡∏á‡∏£‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á: .pt, .pth, .ckpt, .bin (HuggingFace), .safetensors
    """
    try:
        print(f"‚û°Ô∏è ‡∏û‡∏¢‡∏≤‡∏¢‡∏≤‡∏°‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡∏î‡πâ‡∏ß‡∏¢ torch.load(): {path}")
        return torch.load(path, map_location="cpu")
    except Exception as e:
        # ‡∏Å‡∏£‡∏ì‡∏µ safetensors
        if str(path).endswith(".safetensors"):
            print(f"‚û°Ô∏è ‡πÇ‡∏´‡∏•‡∏î‡∏î‡πâ‡∏ß‡∏¢ safetensors: {path}")
            return safetensors.torch.load_file(path, device="cpu")
        # ‡∏ñ‡πâ‡∏≤ error ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏≠‡∏∑‡πà‡∏ô log ‡∏≠‡∏≠‡∏Å‡∏°‡∏≤‡πÉ‡∏´‡πâ‡∏•‡∏∞‡πÄ‡∏≠‡∏µ‡∏¢‡∏î
        size = os.path.getsize(path) if os.path.exists(path) else "N/A"
        raise RuntimeError(
            f"‚ùå ‡πÇ‡∏´‡∏•‡∏î‡πÑ‡∏ü‡∏•‡πå‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÑ‡∏°‡πà‡∏™‡∏≥‡πÄ‡∏£‡πá‡∏à: {type(e).__name__}: {e}\n"
            f"üìÇ Path: {path}\n"
            f"üìè Size: {size} bytes"
        )


def load_model(model_name, model_path, *args, **kwargs):
    """
    Load a PyTorch model from file or URL (safe for state_dict/checkpoint/full model/safetensors)
    """
    # ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å class ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ï‡∏≤‡∏°‡∏ä‡∏∑‡πà‡∏≠
    if "resnet" in model_name.lower():
        from torchvision.models import resnet50
        model_class = lambda: resnet50(num_classes=len(CLASS_NAMES))
    elif "densenet" in model_name.lower():
        from torchvision.models import densenet121
        model_class = lambda: densenet121(num_classes=len(CLASS_NAMES))
    elif "mobilenet" in model_name.lower():
        from torchvision.models import mobilenet_v3_large
        model_class = lambda: mobilenet_v3_large(num_classes=len(CLASS_NAMES))
    elif "efficientnet" in model_name.lower():
        from torchvision.models import efficientnet_b0
        model_class = lambda: efficientnet_b0(num_classes=len(CLASS_NAMES))
    elif "vit" in model_name.lower():
        import timm
        model_class = lambda: timm.create_model(
            'vit_base_patch16_224', pretrained=False, num_classes=len(CLASS_NAMES)
        )
    else:
        raise ValueError("Unknown model type")

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô URL ‡∏´‡∏£‡∏∑‡∏≠‡πÑ‡∏ü‡∏•‡πå local
    if str(model_path).startswith("http"):
        r = requests.get(model_path)
        r.raise_for_status()
        with tempfile.NamedTemporaryFile(delete=False) as f:
            f.write(r.content)
            tmp_path = f.name
    else:
        tmp_path = model_path

    # ‡πÇ‡∏´‡∏•‡∏î state
    state = safe_load_state(tmp_path)

    # ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏ß‡πà‡∏≤ state ‡πÄ‡∏õ‡πá‡∏ô dict ‡πÅ‡∏ö‡∏ö‡πÑ‡∏´‡∏ô
    if isinstance(state, dict) and "state_dict" in state:
        print("‚úÖ checkpoint ‡πÅ‡∏ö‡∏ö Lightning: state['state_dict']")
        model = model_class(*args, **kwargs)
        model.load_state_dict(state["state_dict"], strict=False)
    elif isinstance(state, dict) and any("weight" in k or "bias" in k for k in state.keys()):
        print("‚úÖ state_dict ‡∏õ‡∏Å‡∏ï‡∏¥")
        model = model_class(*args, **kwargs)
        model.load_state_dict(state, strict=False)
    elif isinstance(state, torch.nn.Module):
        print("‚úÖ full model ‡∏ó‡∏µ‡πà save ‡∏ó‡∏±‡πâ‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•")
        model = state
    else:
        raise RuntimeError(f"‚ùå state format ‡πÑ‡∏°‡πà‡∏£‡∏π‡πâ‡∏à‡∏±‡∏Å: {type(state)}")

    model.eval()
    return model
